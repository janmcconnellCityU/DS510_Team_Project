{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f211162",
   "metadata": {},
   "source": [
    "## **Image Classification of Animals Using CNNs and PCA**\n",
    "#### `Dawit Hailu`, `Geraldine Marten-Ellis`, `Jan McConnell`, `Aaron J. Smith`\n",
    "**DS510 Team Project** `Summer 2025`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c0f73",
   "metadata": {},
   "source": [
    "### **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe1bbe",
   "metadata": {},
   "source": [
    "> This project looks at how well Convolutional Neural Networks (CNNs) can classify images of animals and examines whether applying Principal Component Analysis (PCA) improves the results, using a small selection from the High-Resolution Cat-Dog-Bird Image Dataset, with 150 grayscale images in total, divided evenly between cats, dogs, and birds. Our goal is to compare classification accuracy, training time, and loss between models trained on raw images and those preprocessed with PCA. The project highlights how dimensionality reduction can influence both efficiency and generalization in small-scale image classification tasks. Performance metrics and visualizations, including confusion matrices and loss curves will support findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2de90e",
   "metadata": {},
   "source": [
    "*Setup and Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ada774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch torchvision matplotlib scikit-learn pandas numpy scikeras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4527fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491da006",
   "metadata": {},
   "source": [
    "## 1 **CNN Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b21a73",
   "metadata": {},
   "source": [
    "#### 1.1 **Load and Preprocess Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9809958",
   "metadata": {},
   "source": [
    "This class handles reading downloaded CSV datasets, `normalizing` and `reshaping` the image data, `splitting` into training, validation, and test sets, and preparing PyTorch `DataLoader` objects for use in model training class workflows. It also manages label remapping and provides access to data splits, tensors, and class mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6258a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, data_dir=\"./dataset\", dataset_dir = \"mnist-animals-dataset\", batch_size=64, test_size=0.2, random_state=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.class_map = None\n",
    "        # training data\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.x_test = None\n",
    "        self.y_test = None\n",
    "        \n",
    "\n",
    "        self._preprocess()\n",
    "\n",
    "    def _preprocess(self):\n",
    "        # Resize images to 28x28 if not already\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Data directory {self.data_dir} does not exist. Please check the path.\")\n",
    "        \n",
    "        # check if preprocessed data already exists\n",
    "        csv_target = f\"{self.dataset_dir}/mnist-animals.csv\"\n",
    "        if not os.path.exists(csv_target):\n",
    "            # create dataset directory if it does not exist\n",
    "            if not os.path.exists(self.dataset_dir):\n",
    "                os.makedirs(self.dataset_dir)\n",
    "            # write header to csv\n",
    "            with open(csv_target, 'w') as f:\n",
    "                f.write('class,' + ','.join([f'pixel_{i}' for i in range(28 * 28)]) + '\\n')\n",
    "                f.close()\n",
    "            # This loop assumes the images are categorized in subdirectories\n",
    "                # e.g., data_dir/cat, data_dir/dog, data_dir/bird\n",
    "            for subdir in os.listdir(self.data_dir):\n",
    "                subdir_path = os.path.join(self.data_dir, subdir)\n",
    "                for file in os.listdir(subdir_path):\n",
    "                    if file.endswith('.png') or file.endswith('.jpg'):\n",
    "                        file_path = os.path.join(subdir_path, file)\n",
    "                        \n",
    "                        # Here we resize the image to 28x28 and append it to csv\n",
    "                        image = plt.imread(file_path)\n",
    "                        if image.shape[0] != 28 or image.shape[1] != 28:\n",
    "                            resized_image = np.resize(image, (28, 28))\n",
    "                            # add image class to the resized image\n",
    "                            class_label = np.array([subdir])\n",
    "                            resized_image = np.append(class_label, resized_image.flatten())\n",
    "                            \n",
    "                            # write resized image to csv\n",
    "                            with open(csv_target, 'a') as f:\n",
    "                                f.write(','.join(map(str, resized_image)) + '\\n')\n",
    "                                f.close()\n",
    "            print(f\"Preprocessing complete. CSV file saved to {csv_target}\")\n",
    "        else:\n",
    "            print(f\"CSV files already exist in {self.data_dir}/{self.data_dir}. Skipping preprocessing step.\")                  \n",
    "        \n",
    "        # Now we can read the csv file and split it into train and test sets\n",
    "        data = pd.read_csv(csv_target)\n",
    "        self.class_map = {i: label for i, label in enumerate(data['class'].unique())}\n",
    "        X= data.drop(columns=['class']).values\n",
    "        y = data['class'].apply(lambda x: list(self.class_map.keys())[list(self.class_map.values()).index(x)]).values\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n",
    "        \n",
    "\n",
    "    def get_class_map(self):\n",
    "        return self.class_map\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.x_train, self.y_train, self.x_test, self.y_test\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd46a686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files already exist in ./dataset/./dataset. Skipping preprocessing step.\n",
      "Train data shape: (120, 784), Train labels shape: (120,)\n",
      "Test data shape: (30, 784), Test labels shape: (30,)\n",
      "Class map: {0: 'bird', 1: 'cat', 2: 'dog'}\n"
     ]
    }
   ],
   "source": [
    "dataset = DataModule(data_dir=\"./dataset\", dataset_dir=\"mnist-animals\", batch_size=64, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train, y_train, x_test, y_test = dataset.get_train_data()\n",
    "\n",
    "print(f\"Train data shape: {dataset.x_train.shape}, Train labels shape: {dataset.y_train.shape}\")\n",
    "print(f\"Test data shape: {dataset.x_test.shape}, Test labels shape: {dataset.y_test.shape}\")\n",
    "print(f\"Class map: {dataset.class_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25701fe3",
   "metadata": {},
   "source": [
    "### 1.2 **Train and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef622ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davuc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "Epoch 3/40\n",
      "Epoch 4/40\n",
      "Epoch 5/40\n",
      "Epoch 6/40\n",
      "Epoch 7/40\n",
      "Epoch 8/40\n",
      "Epoch 9/40\n",
      "Epoch 10/40\n",
      "Epoch 11/40\n",
      "Epoch 12/40\n",
      "Epoch 13/40\n",
      "Epoch 14/40\n",
      "Epoch 15/40\n",
      "Epoch 16/40\n",
      "Epoch 17/40\n",
      "Epoch 18/40\n",
      "Epoch 19/40\n",
      "Epoch 20/40\n",
      "Epoch 21/40\n",
      "Epoch 22/40\n",
      "Epoch 23/40\n",
      "Epoch 24/40\n",
      "Epoch 25/40\n",
      "Epoch 26/40\n",
      "Epoch 27/40\n",
      "Epoch 28/40\n",
      "Epoch 29/40\n",
      "Epoch 30/40\n",
      "Epoch 31/40\n",
      "Epoch 32/40\n",
      "Epoch 33/40\n",
      "Epoch 34/40\n",
      "Epoch 35/40\n",
      "Epoch 36/40\n",
      "Epoch 37/40\n",
      "Epoch 38/40\n",
      "Epoch 39/40\n",
      "Epoch 40/40\n",
      "Test accuracy: 0.400\n"
     ]
    }
   ],
   "source": [
    "# Reshape and normalize data\n",
    "X_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "X_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "def make_cnn_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(model=make_cnn_model, epochs=40, batch_size=6, verbose=6, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81e8fc",
   "metadata": {},
   "source": [
    "## 2. **PCA Implementation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
